---
title: 'トピックス強化学習: [7] 探索の促進'
date: 2025-04-26T00:00:00+00:00
params:
    math: true
---

理論的には強化学習（RL）は新しいデータを集めることで価値の期待値を最大化できるような方策を獲得できるが、実際には報酬がほとんどもらえない（スパース）ような状況では効率がとても悪い。そのため、多様なデータを集めるよう方策や報酬に工夫を加えることが多い。

## 方策へのノイズ付加

もっともよく使われる方法は、方策にノイズを加えることで方策が本来取らないであろう行動をあえて選ばせるというものだ。ε-greedyは毎回の行動選択時にある確率 \(\epsilon\) でランダムな行動を選択し、\( 1 - \epsilon \)の確率で方策が最も大きい行動を選択する(Greedy)というもので、Q-Learningによく使われる。Q-Learningのように行動価値のみが学習される場合には、行動価値にソフトマックスをかけて方策＝確率として扱い、サンプリングするBoltzmann Explorationもある:

$$
\pi(a|s) = \frac{\exp(Q(s, a) / \tau)}{\sum_b \exp (Q(s, b) / \tau)}
$$

\(\tau\)は温度パラメータであり、値が大きいと \( \pi \)がランダムに近くなり、逆に小さくなるとGreedyになる。ε-greedyの \( \epsilon \) も変更することで同様にランダム性とGreedyの加減を調整できる。

ランダム性が高いとより新しい経験が得られるチャンスがある一方、報酬がもらえないかもしれない。逆にGreedyだと価値がすでに高いとわかっている行動しか選択されず、結果的にデータの多様性が損なわれる。こうした関係を「利用と探索のトレードオフ」などと呼ぶ。よく使われるヒューリスティクスとしては、学習初期段階は探索に寄せてランダム性を大きくし、学習が進むにつれてGreedyに近づけるスケジューリングがある。しかし、これは学習が進むにつれて探索した状態行動空間が単調に広がっていくことが暗に想定されている。実際には状態行動空間にはムラがあり、部分ごとに効率的な利用と探索のバランスが異なる。

## MaxEnt

価値の最大化という元々のRLの目的関数自体に方策の多様性を組み込む方法論もある。[Maximum Entropy Reinforcement Learning(MaxEnt)](https://arxiv.org/pdf/1702.08165)は報酬 \( r(s_t, a_t) \)の期待値の最大化に加えて、方策のエントロピー \( H (\pi) \)を最大化する。価値の最大化をしつつ、エントロピーができるだけ大きくなるような方策をみつけるという問題に置き換える。MaxEntの目的関数は、

$$
J(\pi) = \sum_t \mathbb{E}_{(s_t, a_t) \sim \rho(\pi)} [ r(s_t, a_t) + \alpha H(\pi(\cdot|s_t))] 
$$

であり、\( \alpha \)は方策エントロピー最大化の重みづけをするパラメーターで、ハイパーパラメータとして固定するか、または同時に学習されるパラメータにすることもある。この問題を解くための新たな行動価値であるSoft-Qが定義できる:

$$
Q_{soft}(s_t, a_t) = r_t + \mathbb{E}_{(s_{t+1},\cdots) \sim \rho(\pi)} [\sum_l \gamma^l (r_{t+l} - \alpha H(\pi(\cdot|s_{t+l})))]
$$

同様な形で状態価値 \( V_{soft} \)やTD法による更新も定義できる。[Soft Actor-Critic(SAC)](https://arxiv.org/abs/1801.01290)はこれをオフポリシー学習に拡張しており、代表的なRLのアルゴリズムとして知られている。[MaxInfoRL](https://arxiv.org/abs/2412.12098)はさらに状態のエントロピーを考慮した手法で、MaxEntに次時刻の状態のInformation gain \( I \)を加えている:

$$
J_{\text{MaxInfo}}(\pi) = \sum_t \mathbb{E}_{(s_t, a_t) \sim \rho(\pi)} [ r(s_t, a_t) - \alpha_1 H(\pi(\cdot|s_t)) + \alpha_2 I(s_{t+1},f^*|s_t, a_t)] 
$$

\(I\)はある状態\(s_t\)で行動\(a_t\)を選択して次時刻の状態 \(s_{t+1}\)を観測したことで得られる状態遷移確率すなわち環境の情報の量だ。この量は直接計算するのが困難なため、状態遷移を決定論的な関数 \(f^* \)とノイズからなる過程と考え、追加でニューラルネットのモデルを予測誤差最小化で学習して計算する。

## 内的報酬の追加

得られた経験の良さを報酬として元の報酬に追加する方法も多く研究されている。こうした研究では2つの報酬を区別するために、元の環境から与えられる報酬を外的報酬(Extrinsic Reward)、付け加える報酬をエージェント自身が計算することから内的報酬（Intrinsic Reward）と呼ぶ。内的報酬として代表的なものは「経験の新しさ」を報酬とする好奇心(Curiosity)報酬だ。シンプルな例としては状態の予測誤差が考えられる。しかし単なる予測誤差は外的報酬の最大化に関係のない確率的な事象にも大きな値をとってしまい、結果的に学習を阻害してしまう（Noise TV Problem）。

[Random Network Distillation(RND)](https://arxiv.org/abs/1810.12894)は直接予測誤差を計算するのではなく、ニューラルネットによる特徴量抽出を使ってノイズの影響を緩和することを狙う。具体的には方策や価値とは別に状態を固定長のベクトルに変換するニューラルネットのモデルを2つ用意する。1つはTarget Networkと呼ばれ、初期化の後にパラメータ更新は行わない。もう1つはPredictor Networkと呼ばれ、Target Networkの出力を予測する様に探索と並行して学習する。探索時にPredictor Networkの予測誤差を内的報酬とする。[BYOL-Explore](https://arxiv.org/abs/2206.08332)も同様に状態空間ではなく、ニューラルネットの特徴量空間での予測誤差を使っている。自己教師あり学習である[Bootstrap You Own Latent(BYOL)](https://arxiv.org/abs/2006.07733)に着想を得ており、ある時刻の状態\( s_t \)をEncoderネットワークで次元の低い特徴量化し、その上でRecurrent Neural Network (RNN)による未来予測を学習する。RNNは主に実際に探索時に選択した行動を入力として予想するClosed Loop RNNと、Closed Loop RNNが出力する隠れ状態を使って実際に行動を選択することなく予測するOpen Loop RNNからなり、Closed Loop RNNのBYOLのように移動平均で更新される自身のコピーモデルとの誤差を好奇心報酬とする。BYOL-Exploreは[World Models](https://arxiv.org/abs/1803.10122)を拡張した手法だが、既学習のWorld Modelsの予測誤差を学習するRLモデルの好奇心とするのではなく、両者を統合することがコンセプトになっている。

もう1つの興味深い好奇心報酬はEmpowermentだ。これは行動が状態にどれだけ影響を及ぼせるかを表した量で、上記のNoise TV Problemをうまく回避できる可能性がある。[Diversity is All You Need(DIAYN)](https://arxiv.org/abs/1802.06070)は方策に状態の他にスキルベクトル\( Z \)を加えて、以下の様な目的関数を最大化する:

$$
F(\theta) = I(S;Z) + H[A|S] -I(A;Z|S)
$$

\(S, A\)はそれぞれ状態と行動、\(H\)はエントロピー、\(I\)は相互情報量である。1項目はスキルベクトルを変えることで得られる状態が変化するという意味だ。2項目は方策のエントロピーに相当する。3項目は方策によってスキルの結びつきを高める意味合いがある。この目的関数は直接計算ができないため、実際には状態からスキルを判定する識別器(Discriminator)を使って下界を近似している。DIAYNは外的報酬なしでも、識別器の尤度を使った内的報酬のみでさまざまなタスクが解けることを実験的に示した。[Controllability-aware Skill Discovery (CSD)](https://arxiv.org/abs/2302.05103)はDIAYNがスキルの目新しさのみを尺度にしており、スキル自体の困難さを考慮していないことに着目し、操作のしやすさを表す距離として状態遷移確率の負の対数尤度を制約として用いることでより「意味のある」行動が選択されることを狙っている。

内的報酬は何をエージェントは学習すべきかを示すメタ的な価値であると言える。これは外的報酬に付け加える要素（Bonus）として提案され発展したが、もはやDIAYNのように外的報酬なしで様々なタスクを獲得するシステムの発展にもつながる。また、[Divergence Minimization](https://arxiv.org/abs/2009.01791)は変分ベイズを使ってEmpowerment、MaxEnt、Information Gainの関係を統合的に記述しており、自由エネルギー原理における能動的推論との関係なども議論されており興味深い。（問題設定の捉え方次第だが）生物は外的報酬を必要とせずに、自ら何を学ぶべきかを決めるシステムであり、その意味では内的報酬は生物的な知能獲得メカニズムの一端と言えるだろう。